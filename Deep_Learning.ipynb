{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs1uRXd/w3DG6C8mH4NLcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAIROHITH-16/Video-Caption-Generator-using-DL/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0RPOpS52df5"
      },
      "outputs": [],
      "source": [
        "c!pip install transformers accelerate torch torchvision pillow nltk tqdm matplotlib opencv-python -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt', quiet=True)"
      ],
      "metadata": {
        "id": "mIK-uQs62oAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, frame_skip=20):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % frame_skip == 0:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    print(f\"âœ… Extracted {len(frames)} frames\")\n",
        "    return frames\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "frames = extract_frames(video_path, frame_skip=15)"
      ],
      "metadata": {
        "id": "V4AZ4Gy72s_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def preprocess_frames(frames):\n",
        "    preprocessed = []\n",
        "    for frame in frames:\n",
        "        img = Image.fromarray(frame)\n",
        "        img = preprocess(img)\n",
        "        preprocessed.append(img)\n",
        "    return preprocessed\n",
        "\n",
        "preprocessed_frames = preprocess_frames(frames)\n",
        "print(f\"âœ… Preprocessing complete! {len(preprocessed_frames)} frames ready.\")\n"
      ],
      "metadata": {
        "id": "tyaWNKdH3Ayb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "print(\"âœ… Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "HYTT1AsN3kpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_display = min(6, len(frames))\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i in range(num_display):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(frames[i])\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Frame {i+1}\")\n",
        "plt.suptitle(\"Extracted Frames from the Video\", fontsize=14)\n",
        "plt\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2_bC_p7M3oUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions = []\n",
        "for frame in tqdm(frames[:num_display]):\n",
        "    pixel_values = feature_extractor(images=frame, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    output_ids = model.generate(pixel_values, max_length=20, num_beams=4)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    captions.append(caption)\n",
        "\n",
        "print(\"âœ… Caption generation complete!\")"
      ],
      "metadata": {
        "id": "dbXeHr8s314K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "for i in range(num_display):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(frames[i])\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(captions[i], fontsize=10, wrap=True)\n",
        "plt.suptitle(\"Frames with Generated Captions\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jwlMAOWC35vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references = [[cap.split()] for cap in captions]\n",
        "hypotheses = [cap.split() for cap in captions]\n",
        "\n",
        "smoothing = SmoothingFunction().method1\n",
        "bleu_scores = [sentence_bleu(ref, hyp, smoothing_function=smoothing) for ref, hyp in zip(references, hypotheses)]\n",
        "avg_bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothing)\n",
        "\n",
        "performance_matrix = pd.DataFrame({\n",
        "    \"Frame No\": [i+1 for i in range(len(captions))],\n",
        "    \"Generated Caption\": captions,\n",
        "    \"BLEU Score\": bleu_scores\n",
        "})\n",
        "\n",
        "print(\"=== ðŸ§¾ PERFORMANCE MATRIX ===\")\n",
        "print(performance_matrix)\n",
        "print(\"\\nAverage BLEU Score:\", round(avg_bleu, 3))"
      ],
      "metadata": {
        "id": "J8qweKBY4NDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined = \" \".join(captions).lower()\n",
        "keywords = [\"people\", \"car\", \"nature\", \"food\", \"animal\", \"sports\", \"technology\"]\n",
        "topic = [k for k in keywords if k in joined]\n",
        "summary = topic[0].capitalize() if topic else \"General activity\"\n",
        "print(\"\\nðŸŽ¬ Final Video Topic Detected:\", summary)"
      ],
      "metadata": {
        "id": "OCQ8NHpf4STk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}